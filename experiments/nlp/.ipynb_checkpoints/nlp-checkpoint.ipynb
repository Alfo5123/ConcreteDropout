{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMWithCDropout(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size, gpu_id, weight_regularizer,\n",
    "            dropout_regularizer, init_min, init_max):\n",
    "\n",
    "        super(LSTMWithCDropout, self).__init__()\n",
    "        # Post drop out layer\n",
    "        self.layer = nn.LSTM(input_size, output_size, batch_first=True)\n",
    "        # Input dim for regularisation scaling\n",
    "        self.input_dim = input_size\n",
    "        # Regularisation hyper-parameters\n",
    "        self.weight_regularizer = weight_regularizer\n",
    "        self.dropout_regularizer = dropout_regularizer\n",
    "        # Initialise p_logit\n",
    "        init_min = np.log(init_min) - np.log(1. - init_min)\n",
    "        init_max = np.log(init_max) - np.log(1. - init_max)\n",
    "        self.p_logit = nn.Parameter(torch.Tensor(1))\n",
    "        nn.init.uniform(self.p_logit, a=init_min, b=init_max)\n",
    "\n",
    "        self.gpu_id = gpu_id\n",
    "\n",
    "        self.use_dropout = True\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.use_dropout:\n",
    "            return self.layer(self._concrete_dropout(x))\n",
    "\n",
    "        return self.layer(x)\n",
    "\n",
    "    def regularisation(self):\n",
    "\n",
    "        if not self.use_dropout:\n",
    "            return 0.0\n",
    "\n",
    "        self.p = nn.functional.sigmoid(self.p_logit)\n",
    "        \n",
    "        weights_regularizer = self.weight_regularizer * self.sum_n_square() / (1 - self.p)\n",
    "        dropout_regularizer = self.p * torch.log(self.p)\n",
    "        dropout_regularizer += (1. - self.p) * torch.log(1. - self.p)\n",
    "        dropout_regularizer *= self.dropout_regularizer*self.input_dim\n",
    "        regularizer = weights_regularizer + dropout_regularizer\n",
    "        return regularizer\n",
    "\n",
    "    def _concrete_dropout(self, x):\n",
    "        \"\"\"Forward pass for dropout layer\n",
    "        \"\"\"\n",
    "        eps = 1e-7\n",
    "        temp = 0.1\n",
    "        self.p = nn.functional.sigmoid(self.p_logit)\n",
    "\n",
    "        uniform_distribution = torch.distributions.uniform.Uniform(0, 1)\n",
    "\n",
    "        unif_noise = uniform_distribution.sample(sample_shape=x.size()).cuda(self.gpu_id)\n",
    "\n",
    "        drop_prob = (torch.log(self.p + eps)\n",
    "                    - torch.log(1 - self.p + eps)\n",
    "                    + torch.log(unif_noise + eps)\n",
    "                    - torch.log(1 - unif_noise + eps))\n",
    "        drop_prob = nn.functional.sigmoid(drop_prob / temp)\n",
    "\n",
    "        random_tensor = 1 - drop_prob\n",
    "        retain_prob = 1 - self.p\n",
    "        x  = torch.mul(x, random_tensor)\n",
    "        x /= retain_prob\n",
    "        return x\n",
    "\n",
    "    def sum_n_square(self):\n",
    "        return torch.sum(torch.pow(self.layer.weight_ih_l0, 2)) + \\\n",
    "                torch.sum(torch.pow(self.layer.bias_ih_l0, 2))\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, graphemes_vocab_size, phonemes_vocab_size, seq_length,\n",
    "            dropout):\n",
    "\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        embedding_size = 100\n",
    "        lstm_thickness = 100\n",
    "\n",
    "        self.mode = dropout['mode']\n",
    "\n",
    "        self.embedding = nn.Embedding(graphemes_vocab_size, embedding_size)\n",
    "\n",
    "        if self.mode == 'dropout':\n",
    "            self.encoder_dropout1 = nn.Dropout(dropout_data['prob'])\n",
    "            self.encoder_lstm1 = nn.LSTM(embedding_size, lstm_thickness, batch_first=True)\n",
    "        else:\n",
    "            self.encoder_lstm1 = LSTMWithCDropout(embedding_size, lstm_thickness,\n",
    "                    dropout['gpu_id'], dropout['wr'], dropout['dr'],\n",
    "                    dropout['init_min'], dropout['init_max'])\n",
    "\n",
    "        self.encoder_batchnorm2 = nn.BatchNorm1d(lstm_thickness)\n",
    "\n",
    "        if self.mode == 'dropout':\n",
    "            self.encoder_dropout2 = nn.Dropout(dropout_data['prob'])\n",
    "            self.encoder_lstm2 = nn.LSTM(lstm_thickness, lstm_thickness, batch_first=True)\n",
    "        else:\n",
    "            self.encoder_lstm2 = LSTMWithCDropout(lstm_thickness, lstm_thickness,\n",
    "                    dropout['gpu_id'], dropout['wr'], dropout['dr'],\n",
    "                    dropout['init_min'], dropout['init_max'])\n",
    "\n",
    "        self.encoder_batchnorm3 = nn.BatchNorm1d(lstm_thickness)\n",
    "\n",
    "        if self.mode == 'dropout':\n",
    "            self.encoder_dropout3 = nn.Dropout(dropout_data['prob'])\n",
    "            self.encoder_lstm3 = nn.LSTM(lstm_thickness, lstm_thickness, batch_first=True)\n",
    "        else:\n",
    "            self.encoder_lstm3 = LSTMWithCDropout(lstm_thickness, lstm_thickness,\n",
    "                    dropout['gpu_id'], dropout['wr'], dropout['dr'],\n",
    "                    dropout['init_min'], dropout['init_max'])\n",
    "\n",
    "        self.average_pooling = nn.AvgPool1d(seq_length)\n",
    "\n",
    "        self.decoder_batchnorm1 = nn.BatchNorm1d(lstm_thickness)\n",
    "\n",
    "        if self.mode == 'dropout':\n",
    "            self.decoder_dropout1 = nn.Dropout(dropout_data['prob'])\n",
    "            self.decoder_lstm1 = nn.LSTM(lstm_thickness, lstm_thickness, batch_first=True)\n",
    "        else:\n",
    "            self.decoder_lstm1 = LSTMWithCDropout(lstm_thickness, lstm_thickness,\n",
    "                    dropout['gpu_id'], dropout['wr'], dropout['dr'],\n",
    "                    dropout['init_min'], dropout['init_max'])\n",
    "\n",
    "        self.decoder_batchnorm2 = nn.BatchNorm1d(lstm_thickness)\n",
    "\n",
    "        if self.mode == 'dropout':\n",
    "            self.decoder_dropout2 = nn.Dropout(dropout_data['prob'])\n",
    "            self.decoder_lstm2 = nn.LSTM(lstm_thickness, lstm_thickness, batch_first=True)\n",
    "        else:\n",
    "            self.decoder_lstm2 = LSTMWithCDropout(lstm_thickness, lstm_thickness,\n",
    "                    dropout['gpu_id'], dropout['wr'], dropout['dr'],\n",
    "                    dropout['init_min'], dropout['init_max'])\n",
    "\n",
    "        self.decoder_linear = nn.Linear(lstm_thickness, phonemes_vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # x of shape (batch, seq)\n",
    "\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        if self.mode == 'dropout':\n",
    "            x = self.encoder_dropout1(x)\n",
    "        x = self.encoder_lstm1(x)[0]\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.encoder_batchnorm2(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        if self.mode == 'dropout':\n",
    "            x = self.encoder_dropout2(x)\n",
    "        x = self.encoder_lstm2(x)[0]\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.encoder_batchnorm3(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        if self.mode == 'dropout':\n",
    "            x = self.encoder_dropout3(x)\n",
    "        x = self.encoder_lstm3(x)[0]\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.average_pooling(x)\n",
    "        x = torch.cat([x]*seq_length, 2)\n",
    "\n",
    "        x = self.decoder_batchnorm1(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        if self.mode == 'dropout':\n",
    "            x = self.decoder_dropout1(x)\n",
    "        x = self.decoder_lstm1(x)[0]\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.decoder_batchnorm2(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        if self.mode == 'dropout':\n",
    "            x = self.decoder_dropout2(x)\n",
    "        x = self.decoder_lstm2(x)[0]\n",
    "\n",
    "        x = self.decoder_linear(x)\n",
    "\n",
    "        return nn.functional.log_softmax(x, 2)\n",
    "\n",
    "def get_regularization_loss(model):\n",
    "\n",
    "    regularization_loss = 0.0\n",
    "\n",
    "    def get_module_regularization_loss(module):\n",
    "\n",
    "        nonlocal regularization_loss\n",
    "        \n",
    "        if module.__class__.__name__.endswith('LSTMWithCDropout'):\n",
    "            regularization_loss = regularization_loss + module.regularisation()\n",
    "\n",
    "    model.apply(get_module_regularization_loss)\n",
    "\n",
    "    return regularization_loss\n",
    "\n",
    "def set_dropout_state(model, value):\n",
    "\n",
    "    def set_dropout_state_in_module(module):\n",
    "\n",
    "        if module.__class__.__name__.endswith('LSTMWithCDropout'):\n",
    "            module.use_dropout = value\n",
    "\n",
    "    model.apply(set_dropout_state_in_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_data(data, seq_length):\n",
    "\n",
    "    unique_graphemes = set()\n",
    "    unique_phonemes = set()\n",
    "\n",
    "    for word, transcription in zip(data[:, 0], data[:, 1]):\n",
    "        unique_graphemes |= set(word)\n",
    "        unique_phonemes |= set(transcription.split())\n",
    "\n",
    "    grapheme_codes = {grapheme:i + 1 for i, grapheme in enumerate(unique_graphemes)}\n",
    "    phoneme_codes = {phoneme:i + 1 for i, phoneme in enumerate(unique_phonemes)}\n",
    "\n",
    "    encoded_words = np.zeros((len(data), seq_length), dtype=int)\n",
    "    encoded_transcriptions = np.zeros((len(data), seq_length), dtype=int)\n",
    "\n",
    "    for index, (word, transcription) in enumerate(zip(data[:, 0], data[:, 1])):\n",
    "\n",
    "        encoded_word = [grapheme_codes[grapheme] for grapheme in word]\n",
    "        encoded_transcription = [phoneme_codes[phoneme] for phoneme in transcription.split()]\n",
    "\n",
    "        encoded_words[index, :len(encoded_word)] = encoded_word\n",
    "        encoded_transcriptions[index, :len(encoded_transcription)] = encoded_transcription\n",
    "\n",
    "    return grapheme_codes, phoneme_codes, encoded_words, encoded_transcriptions\n",
    "\n",
    "data = np.array(pd.read_csv('train.csv', delimiter=',', index_col='Id'), dtype=str)\n",
    "\n",
    "seq_length = 40\n",
    "\n",
    "grapheme_codes, phoneme_codes, X, y = vectorize_data(data, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "permutation = np.random.permutation(len(X))\n",
    "\n",
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "\n",
    "train_X = X[:90000]\n",
    "train_y = y[:90000]\n",
    "val_X = X[90000:95000]\n",
    "val_y = y[90000:95000]\n",
    "test_X = X[95000:]\n",
    "test_y = y[95000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u0037/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n"
     ]
    }
   ],
   "source": [
    "epochs_count = 100\n",
    "batch_size = 500\n",
    "gpu_id = 0\n",
    "\n",
    "#dropout_data = {'mode': 'dropout', 'prob': 0.2}\n",
    "dropout_data = {'mode': 'concrete', 'gpu_id': gpu_id, 'wr': 1e-4, 'dr': 1e-4,\n",
    "        'init_min': 0.05, 'init_max': 0.5}\n",
    "\n",
    "model = Model(len(grapheme_codes) + 1, len(phoneme_codes) + 1, seq_length,\n",
    "              dropout_data).cuda(gpu_id)\n",
    "\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=1e-2)\n",
    "\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u0037/miniconda3/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "1.218 minutes passed\n",
      "train-loss=0.598 train-score=5.997 val-loss=0.417 val-score=4.547\n",
      "Epoch 1\n",
      "2.417 minutes passed\n",
      "train-loss=0.386 train-score=4.354 val-loss=0.419 val-score=4.621\n",
      "Epoch 2\n",
      "3.586 minutes passed\n",
      "train-loss=0.309 train-score=3.529 val-loss=0.254 val-score=2.916\n",
      "Epoch 3\n",
      "4.784 minutes passed\n",
      "train-loss=0.251 train-score=2.923 val-loss=0.318 val-score=3.240\n",
      "Epoch 4\n",
      "5.975 minutes passed\n",
      "train-loss=0.214 train-score=2.514 val-loss=0.204 val-score=2.377\n",
      "Epoch 5\n",
      "7.139 minutes passed\n",
      "train-loss=0.187 train-score=2.254 val-loss=0.191 val-score=2.348\n",
      "Epoch 6\n",
      "8.285 minutes passed\n",
      "train-loss=0.191 train-score=2.242 val-loss=3.451 val-score=35.303\n",
      "Epoch 7\n",
      "9.409 minutes passed\n",
      "train-loss=0.205 train-score=2.451 val-loss=0.163 val-score=1.999\n",
      "Epoch 8\n",
      "10.597 minutes passed\n",
      "train-loss=0.164 train-score=2.009 val-loss=0.180 val-score=2.174\n",
      "Epoch 9\n",
      "11.728 minutes passed\n",
      "train-loss=0.155 train-score=1.903 val-loss=0.182 val-score=2.090\n",
      "Epoch 10\n",
      "12.890 minutes passed\n",
      "train-loss=0.146 train-score=1.808 val-loss=0.192 val-score=2.204\n",
      "Epoch 11\n",
      "14.019 minutes passed\n",
      "train-loss=0.142 train-score=1.752 val-loss=0.149 val-score=1.867\n",
      "Epoch 12\n",
      "15.142 minutes passed\n",
      "train-loss=0.138 train-score=1.702 val-loss=0.143 val-score=1.780\n",
      "Epoch 13\n",
      "16.352 minutes passed\n",
      "train-loss=0.132 train-score=1.639 val-loss=0.133 val-score=1.636\n",
      "Epoch 14\n",
      "17.530 minutes passed\n",
      "train-loss=0.128 train-score=1.605 val-loss=0.133 val-score=1.663\n",
      "Epoch 15\n",
      "18.759 minutes passed\n",
      "train-loss=0.128 train-score=1.591 val-loss=0.139 val-score=1.777\n",
      "Epoch 16\n",
      "19.968 minutes passed\n",
      "train-loss=0.125 train-score=1.558 val-loss=0.170 val-score=2.024\n",
      "Epoch 17\n",
      "21.209 minutes passed\n",
      "train-loss=0.121 train-score=1.508 val-loss=0.121 val-score=1.532\n",
      "Epoch 18\n",
      "22.380 minutes passed\n",
      "train-loss=0.121 train-score=1.514 val-loss=0.123 val-score=1.547\n",
      "Epoch 19\n",
      "23.525 minutes passed\n",
      "train-loss=0.118 train-score=1.481 val-loss=0.106 val-score=1.319\n",
      "Epoch 20\n",
      "24.724 minutes passed\n",
      "train-loss=0.127 train-score=1.549 val-loss=0.120 val-score=1.521\n",
      "Epoch 21\n",
      "25.924 minutes passed\n",
      "train-loss=0.114 train-score=1.420 val-loss=0.140 val-score=1.679\n",
      "Epoch 22\n",
      "27.146 minutes passed\n",
      "train-loss=0.116 train-score=1.442 val-loss=0.136 val-score=1.619\n",
      "Epoch 23\n",
      "28.324 minutes passed\n",
      "train-loss=0.112 train-score=1.398 val-loss=0.114 val-score=1.404\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-411848b2d06a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy_batch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-b20cf10170e7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'dropout'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_dropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_lstm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-b20cf10170e7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_dropout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concrete_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-b20cf10170e7>\u001b[0m in \u001b[0;36m_concrete_dropout\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0muniform_distribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0munif_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muniform_distribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         drop_prob = (torch.log(self.p + eps)\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/distributions/uniform.py\u001b[0m in \u001b[0;36mrsample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mrand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrand\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "for epoch in range(epochs_count):\n",
    "\n",
    "    permutation = np.random.permutation(len(train_X))\n",
    "\n",
    "    model.train()\n",
    "    set_dropout_state(model, True)\n",
    "\n",
    "    total_train_loss = 0.0\n",
    "    train_mismatches_count = 0\n",
    "    train_batches_count = 0\n",
    "    \n",
    "    for batch_start in range(0, len(train_X), batch_size):\n",
    "\n",
    "        batch_end = min(len(train_X), batch_start + batch_size)\n",
    "\n",
    "        if batch_start == batch_end:\n",
    "            break\n",
    "\n",
    "        batch_indices = permutation[batch_start:batch_end]\n",
    "\n",
    "        batch_X = train_X[batch_indices]\n",
    "        numpy_batch_y = train_y[batch_indices]\n",
    "\n",
    "        batch_X = Variable(torch.LongTensor(batch_X), requires_grad=False).cuda(gpu_id)\n",
    "        batch_y = Variable(torch.LongTensor(numpy_batch_y), requires_grad=False).cuda(gpu_id)\n",
    "\n",
    "        output = model(batch_X)\n",
    "\n",
    "        train_loss = criterion(output.transpose(1, 2), batch_y)\n",
    "\n",
    "        output = output.detach().cpu().numpy().argmax(axis=2)\n",
    "\n",
    "        train_mismatches_count += (output != numpy_batch_y).sum()\n",
    "        total_train_loss += float(train_loss)\n",
    "        train_batches_count += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        (train_loss + get_regularization_loss(model)).backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    total_train_loss /= train_batches_count\n",
    "    average_train_mismatches = train_mismatches_count/len(train_X)\n",
    " \n",
    "    model.eval()\n",
    "    set_dropout_state(model, False)\n",
    "\n",
    "    val_train_loss = 0.0\n",
    "    val_mismatches_count = 0\n",
    "    val_batches_count = 0\n",
    "\n",
    "    for batch_start in range(0, len(val_X), batch_size):\n",
    "\n",
    "        batch_end = min(len(val_X), batch_start + batch_size)\n",
    "\n",
    "        if batch_start == batch_end:\n",
    "            break\n",
    "\n",
    "        batch_X = val_X[batch_start:batch_end]\n",
    "        numpy_batch_y = val_y[batch_start:batch_end]\n",
    "\n",
    "        batch_X = Variable(torch.LongTensor(batch_X), requires_grad=False).cuda(gpu_id)\n",
    "        batch_y = Variable(torch.LongTensor(numpy_batch_y), requires_grad=False).cuda(gpu_id)\n",
    "\n",
    "        output = model(batch_X)\n",
    "\n",
    "        val_loss = criterion(output.transpose(1, 2), batch_y)\n",
    "\n",
    "        output = output.detach().cpu().numpy().argmax(axis=2)\n",
    "\n",
    "        val_mismatches_count += (output != numpy_batch_y).sum()\n",
    "        val_train_loss += float(val_loss)\n",
    "        val_batches_count += 1\n",
    "\n",
    "    val_train_loss /= val_batches_count\n",
    "    average_val_mismatches = val_mismatches_count/len(val_X)\n",
    "\n",
    "    print('Epoch {}'.format(epoch))\n",
    "    print('{:.3f} minutes passed'.format((time.time() - start)/60))\n",
    "    print('train-loss={0:.3f} train-score={1:.3f} val-loss={2:.3f} val-score={3:.3f}'.format(\n",
    "        total_train_loss, average_train_mismatches, val_train_loss, average_val_mismatches\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
